### Paper Comments
- [Manchingal and Cuzzolin (2022): Epistemic Deep Learning](https://arxiv.org/abs/2206.07609). Very broad overview of many different things. I like the intention of the proposed Epistemic Deep Learning program, but got lost in the paper. Whats worse: I am a bit sceptical about the experimental section since they do not compare themsevles to sota and have no image net. For many approaches that is fine, but here it seems off. 
- [Chouraqui et al. (2022): A Geometric Method for Improved Uncertainty Estimation in Real-time](https://arxiv.org/pdf/2206.11562.pdf). A geometrical approach for uncertainty prediction based on the input-label relationship of classifier models. The exposition is perhaps overly technical, but I still like the approach. Also, I am a bit criticall whether the distance based notions / hyperspheres works well on very high dimensional data. Maybe I am worng. 
- [Guo et al. (2017): On Calibration of Modern Neural Networks](https://proceedings.mlr.press/v70/guo17a.html). Interesting analysis of the uncertainty prediction capacities of many networks. 
- [Zhe Liu et al. (2022): A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness](https://arxiv.org/abs/2205.00403). I like the initial discussion and the motivation of the paper. I think all their intuitions are the right ones. I am not super convinced by the use of GPs and the enforcement of distance awareness into the last layer. 
- [Fannjiang et al. (2022): Conformal Prediction for the Design Problem](https://arxiv.org/abs/2202.03613). Very torough paper discussing an extremely interesting setting: The design problem - setting where the performance on a training set influences the shift on the test-set. 
- [He et al. (2022): NeMF: Neural Motion Fields for Kinematic
Animation](https://arxiv.org/abs/2206.03287). Super clever use of implicit neural representations. I wish I had an use for something like that. 
- [Wolleb et al. (2022): Diffusion Models for Medical Anomaly Detection](https://arxiv.org/abs/2203.04306). Very clever use of diffusion models for anomaly detection. I am not sure about the evaluation though. 
- [Douven (2021): Scoring, Truthlikeness, and Value](https://link.springer.com/article/10.1007/s11229-021-03162-z). A very interesting article about the interellation of model performance in terms of scoring rules and the value the models produce. Douven argue, quite convincingly, that there exist no single scoring rule (proper or not) that perfectly reflects the value of the prediction. 
- [Sousa (2022): Inductive Conformal Prediction: A Straightforward Introduction with Examples in Python](https://arxiv.org/abs/2206.11810). A very brief but very well written introduction to conformal prediction (a type of coverage based uncertaitny prediction for arbitrary models). 
- [Hüllermeier and Wageman (2020): Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods](https://link.springer.com/article/10.1007/s10994-021-05946-3). Perhaps the best introductory paper about uncertainty estimation for ML. The text has both breath and depth. It is well written, very clear explanations and examples. Even touches on conformal predictions and the like.
- Huber (2002): Approximate models. An extremely well written essays about model fitting, robustness and simulation. 
- [Zhang et al. (2021): Understanding plastic degradation and microplastic formation in the environment: A review](https://www.sciencedirect.com/science/article/pii/S0269749121001329). Interesting overview for someone like me who has very little insight to topic. 
- [Wortsman et al. (2022): Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://arxiv.org/abs/2203.05482). Very interesting approach to obtain flat minima. I think i still prefer ensembles, but with the very large model paradigm we are in these might be the next-best thing.
- [Simon (1995): Artificial Intelligence: an empirical science](https://www.sciencedirect.com/science/article/pii/000437029500039H). An AI landmark paper before the advent of Deep Learning. A long read and at times a bit outdates. However, on a larger level it provides some very sharp arguments that are still relevant for todays discussion. 
- [Bengs, Hüllermeier, and Wageman (2022): On the Difficulty of Epistemic Uncertainty Quantification in Machine Learning: The Case of Direct Uncertainty Estimation through Loss Minimisation]. Very cool paper, which shows that is is difficult (and sometimes perhpas impossible) to learn higher order uncetainties (here already 2nd order uncertainty, i.e. Bayesian).
- [Post et al. (2021): Application of Laser-Induced, Deep UV Raman Spectroscopy and Artificial Intelligence in Real-Time Environmental Monitoring—Solutions and First Results](https://www.mdpi.com/1424-8220/21/11/3911). A paper that lines out pathways for ubiquitous microplastic monitoring. Intersting topic, but boring read. 
- [Petropolous et al. (2022): Forecasting: theory and practice](https://www.sciencedirect.com/science/article/pii/S0169207021001758?via%3Dihub). Because of its size and scope it seems pretty useless to me. Maybe a good reference. 
- [Tatsunami and Taki (2022): Sequencer: Deep LSTM for Image Classification](https://arxiv.org/abs/2205.01972). One of these papers that corroborate that with large amounts of data the learning algorithms matter less and less. In this case they show that LSTMs obtain competitive performance for large-scale image data. Very impressive engineering.
- [Cha, Chun, Lee, Cho, Park, Lee, and Park (2021): SWAD: Domain Generalization by Seeking Flat Minima](https://proceedings.neurips.cc/paper/2021/hash/bcb41ccdc4363c6848a1d760f26c28a0-Abstract.html). A flat-minima paper. They show a quite interesting technique to find flat-minima and empirical evidence that flat minima tend to generalize better than sharp ones. 
- [Shi, Daunhawer, Vogt, Torr, and Sanyal (2022): How robust are pre-trained models to distribution shift?](https://arxiv.org/abs/2206.08871): Empirical evidence that unsupervised models outperform supervised models regarding OOD generalization. 
- [Yiou, Jézéquel, Naveau, Otto, Vautard, and Vrac (2017): A statistical framework for conditional extreme event attribution](https://ascmo.copernicus.org/articles/3/17/2017/). An overview of a method to use counterfactual-analysis for assessing whether kinds of extreme events have become more or less likely due to Climate Change. Some things in there are hard to swallow, but great concept. 
- [Lloyd, Oreskes (2019): Climate Change Attribution: When Does it Make Sense to Add Methods?](https://www.pdcnet.org/eps/content/eps_2019_0056_0001_0185_0201). This contribuztion is easy to read and gives a great overview of different appraoches to assess the impact of climate change on extreme events. For my taste it is a bit to opinionated and almost prosaic from time to time. Still worth the read. 
- [Katz, Parlange, and Naveau (2002): Statistics of extremes in hydrology](https://doi.org/10.1016/S0309-1708(02)00056-8). A good overview of the uses of extreme value theory in hydrology. Perhaps needs an udpate. 
- [Foorgione, Muni, Piga, and Gallieri (2022): On the adaptation of recurrent neural networks for system identification](https://arxiv.org/abs/2201.08660). Interstingly enough system identificaiton is an interesting, but for ML unconventional, setting. The idea of the authors is to use that setting to study how RNNs that have been used for system identification purposes can be adjusted to eventual changes in the underlyign system as fast as possible. They propose to use an approximation to an updated model using a sort-of taylor expansion technique. This approximation yields a linearization of the loss-landscape around the current model parameters and lets them directly estimate a potential updade in a Kalman-like forward step. The exposition is good and the results are convincing.  
- [Baartman, Melsen, Moore, and van der Ploeg (2020): On the complexity of model complexity: Viewpoints across the geosciences](https://doi.org/10.1016/j.catena.2019.104261). This contribution gives an interesting take on model complexity. Instead of going trough formal or classical definitions of complexity the authors decided to design a questionaire and actually ask geo-scientist. Instead of deriving yet another definition of model complexity (which would probably be limited, like all current one), the authors emphasize that their results reflect that complexity is a context dependent property. I am not sure if I buy that. 
- [Gontijo-Lopes, Dauphin, and Cubuk (2021): No One Representation to Rule Them All: Overlapping Features of Training Methods](https://arxiv.org/abs/2110.12899). For me this was one of the most interseeting paper at neurips2021. The authors show that one can build super diverse ensembles with neural networks, if the same network is trained with different enough training methods (new self-supervised, state-of-the-art approaches lend themselves more to that than plain supervised ones). 
- [Khintchine (1934): Korrelationstheorie der stationären stochastischen Prozesse](https://link.springer.com/article/10.1007/BF01449156). One of the earliest paper that pins down the concept of stationarity and instationarity. Requires knowledge of german and some patience. Quite precise. 
- [Koutsoyiannis and Sargentis (2021): Entropy and Wealth Demetris](https://www.mdpi.com/1099-4300/23/10/1356). A mathematical essay on the connection between entropy and wealth. True out of the box thinking. 
- [Beven (2020): The era of infiltration
Keith](https://doi.org/10.5194/hess-25-851-2021). Historical inquiry about infiltration theory by one of the most important hydrologist alive. Worth reading for everyone who is itnerested in the history of quantitative hydrology.
- [Merz et al. (2021): Causes, impacts and patterns of disastrous river floods](https://doi.org/10.1038/s43017-021-00195-3). QUite superficial review about the flood risks. Maybe good as a reference. 
- [Saltelli (2020): Ethics of quantification or quantification of ethics?](https://doi.org/10.1016/j.futures.2019.102509). I guess this is what "philosophy of data-science" looks like if it is done by a well read data-scientist (as opposed to a data-sciency philosopher)
- [Voita and Titov (2020)](https://arxiv.org/abs/2003.12298). Probing is cool and this is an excellent paper about how to make probes. 
- [Jain et al. (2021)](https://openreview.net/pdf?id=Jep2ykGUdS). A very cool paper about a quite involved method for uncertainty-estimation based active learning. 
- [Karandikar et al. (2021): Soft Calibration Objectives for Neural Networks](https://papers.nips.cc/paper/2021/file/f8905bd3df64ace64a68e154ba72f24c-Paper.pdf). I do have a heart for soft-appraoches and this seems to be a good idea. If I would actually work on calibration task I would try it for sure. 
- [Biloš et al. (2021): Neural Flows: Efficient Alternative to Neural ODEs](https://arxiv.org/abs/2110.13040). The paper presents an intersting alternative to neuralODE. I am (still) not sure if neuralODE are a good idea in the first place, but here is already an approach that might supersedes them. 
- [Judd and Stemler (2010): Forecasting: it is not about statistics, it is about dynamics](https://www.jstor.org/stable/25663247). When reading one can see that authors had very specific model classes in mind. Still, I quite like their framing and the (historical) view they present regarding the forecasting problem. 
- [Judd and Nakamura (2006): Degeneracy of time series models: The best model is not always the correct model](https://doi.org/10.1063/1.2213957). A very interesting paper that demonstrates on basis of a simple example that when we model a non-linear system and noisy measruements the best model is likely not the correct (i.e. true) model. Given the few citation it has, I think that the insight of the paper and the clearness of the example is underapreciated. 

![reading-papers](https://github.com/allokkio/openLearning/blob/master/img/paper-ideas.png?raw=true)
