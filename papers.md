### Paper Comments
- [Petropolous et al. (2022): Forecasting: theory and practice](https://www.sciencedirect.com/science/article/pii/S0169207021001758?via%3Dihub). Because of its size and scope it seems pretty useless to me. Maybe a good reference. 
- [Tatsunami, Taki (2022): Sequencer: Deep LSTM for Image Classification](https://arxiv.org/abs/2205.01972). One of these papers that corroborate that with large amounts of data the learning algorithms matter less and less. In this case they show that LSTMs obtain competitive performance for large-scale image data. Very impressive engineering.
- [Cha, Chun, Lee, Cho, Park, Lee, Park (2021): SWAD: Domain Generalization by Seeking Flat Minima](https://proceedings.neurips.cc/paper/2021/hash/bcb41ccdc4363c6848a1d760f26c28a0-Abstract.html). A flat-minima paper. They show a quite interesting technique to find flat-minima and empirical evidence that flat minima tend to generalize better than sharp ones. 
- [Shi, Daunhawer, Vogt, Torr, Sanyal (2022): How robust are pre-trained models to distribution shift?](https://arxiv.org/abs/2206.08871): Empirical evidence that unsupervised models outperform supervised models regarding OOD generalization. 
- [Yiou, Jézéquel, Naveau, Otto, Vautard, Vrac (2017): A statistical framework for conditional extreme event attribution](https://ascmo.copernicus.org/articles/3/17/2017/). An overview of a method to use counterfactual-analysis for assessing whether kinds of extreme events have become more or less likely due to Climate Change. Some things in there are hard to swallow, but great concept. 
- [Lloyd, Oreskes (2019): Climate Change Attribution: When Does it Make Sense to Add Methods?](https://www.pdcnet.org/eps/content/eps_2019_0056_0001_0185_0201). This contribuztion is easy to read and gives a great overview of different appraoches to assess the impact of climate change on extreme events. For my taste it is a bit to opinionated and almost prosaic from time to time. Still worth the read. 
- [Katz, Parlange, Naveau (2002): Statistics of extremes in hydrology](https://doi.org/10.1016/S0309-1708(02)00056-8). A good overview of the uses of extreme value theory in hydrology. Perhaps needs an udpate. 
- [Foorgione, Muni, Piga, Gallieri (2022): On the adaptation of recurrent neural networks for system identification](https://arxiv.org/abs/2201.08660). Interstingly enough system identificaiton is an interesting, but for ML unconventional, setting. The idea of the authors is to use that setting to study how RNNs that have been used for system identification purposes can be adjusted to eventual changes in the underlyign system as fast as possible. They propose to use an approximation to an updated model using a sort-of taylor expansion technique. This approximation yields a linearization of the loss-landscape around the current model parameters and lets them directly estimate a potential updade in a Kalman-like forward step. The exposition is good and the results are convincing.  
- [Baartman, Melsen, Moore, van der Ploeg (2020): On the complexity of model complexity: Viewpoints across the geosciences](https://doi.org/10.1016/j.catena.2019.104261). This contribution gives an interesting take on model complexity. Instead of going trough formal or classical definitions of complexity the authors decided to design a questionaire and actually ask geo-scientist. Instead of deriving yet another definition of model complexity (which would probably be limited, like all current one), the authors emphasize that their results reflect that complexity is a context dependent property. I am not sure if I buy that. 
- [Gontijo-Lopes, Dauphin, Cubuk (2021): No One Representation to Rule Them All: Overlapping Features of Training Methods](https://arxiv.org/abs/2110.12899). For me this was one of the most interseeting paper at neurips2021. The authors show that one can build super diverse ensembles with neural networks, if the same network is trained with different enough training methods (new self-supervised, state-of-the-art approaches lend themselves more to that than plain supervised ones). 
- [Khintchine (1934): Korrelationstheorie der stationären stochastischen Prozesse](https://link.springer.com/article/10.1007/BF01449156). One of the earliest paper that pins down the concept of stationarity and instationarity. Requires knowledge of german and some patience. Quite precise. 
- [Koutsoyiannis and Sargentis (2021): Entropy and Wealth Demetris](https://www.mdpi.com/1099-4300/23/10/1356). A mathematical essay on the connection between entropy and wealth. True out of the box thinking. 
- [Beven (2020): The era of infiltration
Keith](https://doi.org/10.5194/hess-25-851-2021). Historical inquiry about infiltration theory by one of the most important hydrologist alive. Worth reading for everyone who is itnerested in the history of quantitative hydrology.
- [Merz et al. (2021): Causes, impacts and patterns of disastrous river floods](https://doi.org/10.1038/s43017-021-00195-3). QUite superficial review about the flood risks. Maybe good as a reference. 
- [Saltelli (2020): Ethics of quantification or quantification of ethics?](https://doi.org/10.1016/j.futures.2019.102509). I guess this is what "philosophy of data-science" looks like if it is done by a well read data-scientist (as opposed to a data-sciency philosopher)
- [Voita and Titov (2020)](https://arxiv.org/abs/2003.12298). Probing is cool and this is an excellent paper about how to make probes. 
- [Jain et al. (2021)](https://openreview.net/pdf?id=Jep2ykGUdS). A very cool paper about a quite involved method for uncertainty-estimation based active learning. 
- [Karandikar et al. (2021): Soft Calibration Objectives for Neural Networks](https://papers.nips.cc/paper/2021/file/f8905bd3df64ace64a68e154ba72f24c-Paper.pdf). I do have a heart for soft-appraoches and this seems to be a good idea. If I would actually work on calibration task I would try it for sure. 
- [Biloš et al. (2021): Neural Flows: Efficient Alternative to Neural ODEs](https://arxiv.org/abs/2110.13040). The paper presents an intersting alternative to neuralODE. I am (still) not sure if neuralODE are a good idea in the first place, but here is already an approach that might supersedes them. 
- [Judd and Stemler (2010): Forecasting: it is not about statistics, it is about dynamics](https://www.jstor.org/stable/25663247). When reading one can see that authors had very specific model classes in mind. Still, I quite like their framing and the (historical) view they present regarding the forecasting problem. 
- [Judd and Nakamura (2006): Degeneracy of time series models: The best model is not always the correct model](https://doi.org/10.1063/1.2213957). A very interesting paper that demonstrates on basis of a simple example that when we model a non-linear system and noisy measruements the best model is likely not the correct (i.e. true) model. Given the few citation it has, I think that the insight of the paper and the clearness of the example is underapreciated. 

![reading-papers](https://github.com/allokkio/openLearning/blob/master/img/paper-ideas.png?raw=true)
