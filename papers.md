### Paper Comments
- [Gontijo-Lopes, Dauphin, Cubuk (2021): No One Representation to Rule Them All: Overlapping Features of Training Methods](https://arxiv.org/abs/2110.12899). For me this was one of the most interseeting paper at neurips2021. The authors show that one can build super diverse ensembles with neural networks, if the same network is trained with different enough training methods (new self-supervised, state-of-the-art approaches lend themselves more to that than plain supervised ones). 
- [Khintchine (1934): Korrelationstheorie der stationären stochastischen Prozesse](https://link.springer.com/article/10.1007/BF01449156). One of the earliest paper that pins down the concept of stationarity and instationarity. Requires knowledge of german and some patience. Quite precise. 
- [Koutsoyiannis and Sargentis (2021): Entropy and Wealth Demetris](https://www.mdpi.com/1099-4300/23/10/1356). A mathematical essay on the connection between entropy and wealth. True out of the box thinking. 
- [Beven (2020): The era of infiltration
Keith](https://doi.org/10.5194/hess-25-851-2021). Historical inquiry about infiltration theory by one of the most important hydrologist alive. Worth reading for everyone who is itnerested in the history of quantitative hydrology.
- [Merz et al. (2021): Causes, impacts and patterns of disastrous river floods](https://doi.org/10.1038/s43017-021-00195-3). QUite superficial review about the flood risks. Maybe good as a reference. 
- [Saltelli (2020): Ethics of quantification or quantification of ethics?](https://doi.org/10.1016/j.futures.2019.102509). I guess this is what "philosophy of data-science" looks like if it is done by a well read data-scientist (as opposed to a data-sciency philosopher)
- [Voita and Titov (2020)](https://arxiv.org/abs/2003.12298). Probing is cool and this is an excellent paper about how to make probes. 
- [Jain et al. (2021)](https://openreview.net/pdf?id=Jep2ykGUdS). A very cool paper about a quite involved method for uncertainty-estimation based active learning. 
- [Karandikar et al. (2021): Soft Calibration Objectives for Neural Networks](https://papers.nips.cc/paper/2021/file/f8905bd3df64ace64a68e154ba72f24c-Paper.pdf). I do have a heart for soft-appraoches and this seems to be a good idea. If I would actually work on calibration task I would try it for sure. 
- [Biloš et al. (2021): Neural Flows: Efficient Alternative to Neural ODEs](https://arxiv.org/abs/2110.13040). The paper presents an intersting alternative to neuralODE. I am (still) not sure if neuralODE are a good idea in the first place, but here is already an approach that might supersedes them. 
- [Judd and Stemler (2010): Forecasting: it is not about statistics, it is about dynamics](https://www.jstor.org/stable/25663247). When reading one can see that authors had very specific model classes in mind. Still, I quite like their framing and the (historical) view they present regarding the forecasting problem. 
- [Judd and Nakamura (2006): Degeneracy of time series models: The best model is not always the correct model](https://doi.org/10.1063/1.2213957). A very interesting paper that demonstrates on basis of a simple example that when we model a non-linear system and noisy measruements the best model is likely not the correct (i.e. true) model. Given the few citation it has, I think that the insight of the paper and the clearness of the example is underapreciated. 

![reading-papers](https://github.com/allokkio/openLearning/blob/master/img/paper-ideas.png?raw=true)
